{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363b13d1-5619-4db8-adc8-2f5bcef4bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n",
      "LGBM MSE: 0.2148\n",
      "XGB MSE: 0.2246\n",
      "CatBoost MSE: 0.1989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# 加载数据\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化模型\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "cat_model = cb.CatBoostRegressor(verbose=0, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "lgb_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "lgb_preds = lgb_model.predict(X_test)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "cat_preds = cat_model.predict(X_test)\n",
    "\n",
    "# 计算MSE\n",
    "lgb_mse = mean_squared_error(y_test, lgb_preds)\n",
    "xgb_mse = mean_squared_error(y_test, xgb_preds)\n",
    "cat_mse = mean_squared_error(y_test, cat_preds)\n",
    "\n",
    "print(f\"LGBM MSE: {lgb_mse:.4f}\")\n",
    "print(f\"XGB MSE: {xgb_mse:.4f}\")\n",
    "print(f\"CatBoost MSE: {cat_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd0f17c-597b-41c7-8dd2-e5095a5a8428",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 13209, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.066917\n",
      "Fold 1 mse: 0.2406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 13209, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073789\n",
      "Fold 2 mse: 0.2166\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 13210, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071794\n",
      "Fold 3 mse: 0.2116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 13210, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.077093\n",
      "Fold 4 mse: 0.2291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 13210, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.070142\n",
      "Fold 5 mse: 0.2089\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n",
      "Average mse: 0.2214 (+/- 0.0118)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_predictions': array([1.34408973, 3.52637272, 2.15333585, ..., 1.97501915, 2.62226002,\n",
       "        3.28115353]),\n",
       " 'test_predictions': array([0.59301304, 0.95281747, 4.89589475, ..., 5.17097376, 0.63120748,\n",
       "        1.77517752]),\n",
       " 'cv_metrics': [0.2405911968967214,\n",
       "  0.21664349660529608,\n",
       "  0.2115931870292395,\n",
       "  0.2290636159764674,\n",
       "  0.20890274019987193],\n",
       " 'cv_mean': 0.22135884734151925,\n",
       " 'cv_std': 0.011849084252854303}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_model_regression_cv(lgb_model, X_train, X_test, y_train, cv=5, seed=42, verbose=1, metric=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1ebca2-5a0d-4dd5-8037-9b8fbed1ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def single_model_regression_cv(model, X_train, X_test, y_train, cv=5, seed=42, verbose=1, metric=\"r2\"):\n",
    "    np.random.seed(seed)\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Choose the metric\n",
    "    if metric == \"r2\":\n",
    "        scorer = r2_score\n",
    "    elif metric == \"mse\":\n",
    "        scorer = mean_squared_error\n",
    "    elif metric == \"mae\":\n",
    "        scorer = mean_absolute_error\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported metric. Use 'r2', 'mse', or 'mae'.\")\n",
    "    \n",
    "    # Initialize lists to store fold results\n",
    "    metrics = []\n",
    "    train_predictions = np.zeros(len(X_train))\n",
    "    test_predictions = np.zeros(len(X_test))\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), 1):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        # Clone and fit the model\n",
    "        cloned_model = clone(model)\n",
    "        cloned_model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predictions\n",
    "        val_pred = cloned_model.predict(X_val_fold)\n",
    "        train_predictions[val_idx] = val_pred\n",
    "        \n",
    "        # Evaluate metric\n",
    "        fold_metric = scorer(y_val_fold, val_pred)\n",
    "        metrics.append(fold_metric)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Fold {fold} {metric}: {fold_metric:.4f}\")\n",
    "    \n",
    "    # Predict on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    # Print average metric\n",
    "    if verbose:\n",
    "        print(f\"Average {metric}: {np.mean(metrics):.4f} (+/- {np.std(metrics):.4f})\")\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        \"train_predictions\": train_predictions,\n",
    "        \"test_predictions\": test_predictions,\n",
    "        \"cv_metrics\": metrics,\n",
    "        \"cv_mean\": np.mean(metrics),\n",
    "        \"cv_std\": np.std(metrics)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baaae3-e249-4a57-8ff9-1f0f11b70123",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
